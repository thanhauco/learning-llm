# Core Brainpower - Tokenization, Embeddings & Attention

## What You'll Learn

1. **Tokenization** - How text becomes numbers
2. **Embeddings** - Semantic similarity and vector math
3. **Attention Mechanisms** - KV cache and transformer internals

## Key Concepts

- BPE, WordPiece, SentencePiece tokenizers
- Cosine similarity, dot product, L2 distance
- Self-attention, multi-head attention, KV caching
- Token limits and context windows

## Files

- `easy.py` - Basic tokenization and embeddings
- `intermediate.py` - Similarity search and attention visualization
- `advanced.py` - Custom tokenizers and KV cache simulation
